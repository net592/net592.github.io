{"pages":[{"title":"about-me","text":"My is name zeming Braveheart … 1234* Born in 90s* From China Jiaozuo* Love Dream Life Tech* A Network &amp; Operations &amp; Development Worker","path":"about-me/index.html"}],"posts":[{"title":"[K8s 1.9实践]Kubeadm HA 1.9 高可用 集群 本地离线部署","text":"Kubeadm HA 1.9 高可用 集群 本地离线部署 k8s 发展速度很快，目前很多大的公司容器集群都基于该项目，如京东，腾讯，滴滴，瓜子二手车等等。 kubernetes1.9版本发布2017年12月15日，每是那三个月一个迭代， Workloads API成为稳定版本，这消除了很多潜在用户对于该功能稳定性的担忧。还有一个重大更新，就是测试支持了Windows了，这打开了在kubernetes中运行Windows工作负载的大门。 CoreDNS alpha可以使用标准工具来安装CoreDNS kube-proxy的IPVS模式进入beta版，为大型集群提供更好的可扩展性和性能。 kube-router的网络插件支持，更方便进行路由控制，发布，和安全策略管理 k8s 核心架构如架构图 k8s 高可用2个核心 ==apiserver master== and ==etcd== ==apiserver master==：（需高可用）集群核心，集群API接口、集群各个组件通信的中枢；集群安全控制； ==etcd== ：（需高可用）集群的数据中心，用于存放集群的配置以及状态信息，非常重要，如果数据丢失那么集群将无法恢复；因此高可用集群部署首先就是etcd是高可用集群； kube-scheduler：调度器 （内部自选举）集群Pod的调度中心；默认kubeadm安装情况下–leader-elect参数已经设置为true，保证master集群中只有一个kube-scheduler处于活跃状态； kube-controller-manager： 控制器 （内部自选举）集群状态管理器，当集群状态与期望不同时，kcm会努力让集群恢复期望状态，比如：当一个pod死掉，kcm会努力新建一个pod来恢复对应replicas set期望的状态；默认kubeadm安装情况下–leader-elect参数已经设置为true，保证master集群中只有一个kube-controller-manager处于活跃状态； kubelet: agent node注册apiserver kube-proxy: 每个node上一个，负责service vip到endpoint pod的流量转发，老版本主要通过设置iptables规则实现，新版1.9基于kube-proxy-lvs 实现 部署示意图 集群ha方案，我们力求简单，使用keepalive 监听一个vip来实现，（当节点不可以后，会有vip漂移的切换时长，取决于我们设置timeout切换时长，测试会有10s空档期，如果对高可用更高要求 可以用lvs或者nginx做 4层lb负载 更佳完美，我们力求简单够用，可接受10s的api不可用） 部署环境最近在部署k8s 1.9集群遇到一些问题，整理记录，或许帮一些朋友。 因为kubeadm 简单便捷，所以集群基于该项目部署，目前bete版本不支持ha部署，github说2018年预计发布ha 版本，可我们等不及。。。 环境 版本 Centos CentOS Linux release 7.3.1611 (Core) Kernel Linux etcd-host1 3.10.0-514.el7.x86_64 yum base repo http://mirrors.aliyun.com/repo/Centos-7.repo yum epel repo http://mirrors.aliyun.com/repo/epel-7.repo kubectl v1.9.0 kubeadmin v1.9.0 docker 1.12.6 docker localre devhub.beisencorp.com 主机名称 相关信息 备注 etcd-host1 10.129.6.211 master和etcd etcd-host2 10.129.6.212 master和etcd etcd-host3 10.129.6.213 master和etcd Vip-keepalive 10.129.6.220 vip用于高可用 环境部署 （我们使用本地离线镜像）环境预初始化 Centos Mini安装 每台机器root 设置机器名 1hostnamectl set-hostname etcd-host1 停防火墙 123systemctl stop firewalldsystemctl disable firewalldsystemctl disable firewalld 关闭Swap 12swapoff -a sed &apos;s/.*swap.*/#&amp;/&apos; /etc/fstab 关闭防火墙 1systemctl disable firewalld &amp;&amp; systemctl stop firewalld &amp;&amp; systemctl status firewalld 关闭Selinux 1234567setenforce 0 sed -i &quot;s/^SELINUX=enforcing/SELINUX=disabled/g&quot; /etc/sysconfig/selinux sed -i &quot;s/^SELINUX=enforcing/SELINUX=disabled/g&quot; /etc/selinux/config sed -i &quot;s/^SELINUX=permissive/SELINUX=disabled/g&quot; /etc/sysconfig/selinux sed -i &quot;s/^SELINUX=permissive/SELINUX=disabled/g&quot; /etc/selinux/config getenforce 增加DNS 1echo nameserver 114.114.114.114&gt;&gt;/etc/resolv.conf 设置内核 123456789101112131415cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl -p /etc/sysctl.conf#若问题执行sysctl -p 时出现：sysctl -psysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-ip6tables: No such file or directorysysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory 解决方法：modprobe br_netfilterls /proc/sys/net/bridge 配置VIP Master keepalived 到目前为止,三个master节点 相互独立运行,互补干扰. kube-apiserver作为核心入口, 可以使用keepalived 实现高可用, kubeadm join暂时不支持负载均衡的方式,所以我们 安装 1yum install -y keepalived 配置keepalived.conf 123456789101112131415161718192021222324252627282930313233343536373839404142cat &gt;/etc/keepalived/keepalived.conf &lt;&lt;EOLglobal_defs &#123; router_id LVS_k8s&#125;vrrp_script CheckK8sMaster &#123; script &quot;curl -k https://10.129.6.220:6443&quot; interval 3 timeout 9 fall 2 rise 2&#125;vrrp_instance VI_1 &#123; state MASTER interface ens32 virtual_router_id 61 # 主节点权重最高 依次减少 priority 120 advert_int 1 #修改为本地IP mcast_src_ip 10.129.6.211 nopreempt authentication &#123; auth_type PASS auth_pass sqP05dQgMSlzrxHj &#125; unicast_peer &#123; #注释掉本地IP #10.129.6.211 10.129.6.212 10.129.6.213 &#125; virtual_ipaddress &#123; 10.129.6.220/24 &#125; track_script &#123; CheckK8sMaster &#125;&#125;EOL 启动 1systemctl enable keepalived &amp;&amp; systemctl restart keepalived 结果 123456789101112[root@etcd-host1 k8s]# systemctl status keepalived● keepalived.service - LVS and VRRP High Availabilitymonitor Loaded: loaded (/usr/lib/systemd/system/keepalived.service; enabled; vendor preset: disabled) Active: active (running) since Fri 2018-01-19 10:27:58 CST; 8h ago Main PID: 1158 (keepalived) CGroup: /system.slice/keepalived.service ├─1158 /usr/sbin/keepalived -D ├─1159 /usr/sbin/keepalived -D └─1161 /usr/sbin/keepalived -DJan 19 10:28:00 etcd-host1 Keepalived_vrrp[1161]: Sending gratuitous ARP on ens32 for 10.129.6.220Jan 19 10:28:05 etcd-host1 Keepalived_vrrp[1161]: VRRP_Instance(VI_1) Sending/queueing gratuitous ARPs on ens32 for 10.129.6.220 依次配置 其他2台从节点master 配置 提取k8s rpm 包 默认由于某某出海问题 我们离线导入下rpm 仓库 安装官方YUM 仓库 12345678910cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=0gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpgEOF 安装kubeadm kubectl cni 下载镜像(自行搬梯子先获取rpm) 123456789mkdir -p /root/k8s/rpmcd /root/k8s/rpm#安装同步工具yum install -y yum-utils#同步本地镜像yumdownloader kubelet kubeadm kubectl kubernetes-cni dockerscp root@10.129.6.224:/root/k8s/rpm/* /root/k8s/rpm 离线安装 123mkdir -p /root/k8s/rpmscp root@10.129.6.211:/root/k8s/rpm/* /root/k8s/rpmyum install /root/k8s/rpm/*.rpm -y 启动k8s 123# restartsystemctl enable docker &amp;&amp; systemctl restart dockersystemctl enable kubelet &amp;&amp; systemctl restart kubelet Etcd https 集群部署（省略后期补充）1步骤过多计划，后期完善下 镜像获取方法 加速器获取 gcr.io k8s镜像 ，导出，导入镜像 或 上传本地仓库 123456789101112国内可以使用daocloud加速器下载相关镜像，然后通过docker save、docker load把本地下载的镜像放到kubernetes集群的所在机器上，daocloud加速器链接如下：https://www.daocloud.io/mirror#accelerator-doc#pull 获取docker pull gcr.io/google_containers/kube-proxy-amd64:v1.9.0#导出mkdir -p docker-imagesdocker save -o docker-images/kube-proxy-amd64 gcr.io/google_containers/kube-proxy-amd64:v1.9.0#导入docker load -i /root/kubeadm-ha/docker-images/kube-proxy-amd64 代理或vpn获取 gcr.io k8s镜 ,导出，导入镜像 或 上传本地仓库 1自谋生路，天机屋漏 kubelet 指定本地镜像kubelet 修改 配置以使用本地自定义pause镜像devhub.beisencorp.com/google_containers/pause-amd64:3.0 替换你的环境镜像123456cat &gt; /etc/systemd/system/kubelet.service.d/20-pod-infra-image.conf &lt;&lt;EOF[Service]Environment=&quot;KUBELET_EXTRA_ARGS=--pod-infra-container-image=devhub.beisencorp.com/google_containers/pause-amd64:3.0&quot;EOFsystemctl daemon-reloadsystemctl restart kubelet Kubeadm Init 初始化 我们使用config 模板方式来初始化集群，便于我们指定etcd 集群 12345678910111213141516171819202122232425262728293031cat &lt;&lt;EOF &gt; config.yaml apiVersion: kubeadm.k8s.io/v1alpha1kind: MasterConfigurationetcd: endpoints: - https://10.129.6.211:2379 - https://10.129.6.212:2379 - https://10.129.6.213:2379 caFile: /etc/etcd/ssl/ca.pem certFile: /etc/etcd/ssl/etcd.pem keyFile: /etc/etcd/ssl/etcd-key.pem dataDir: /var/lib/etcdnetworking: podSubnet: 10.244.0.0/16kubernetesVersion: 1.9.0api: advertiseAddress: &quot;10.129.6.220&quot;token: &quot;b99a00.a144ef80536d4344&quot;tokenTTL: &quot;0s&quot;apiServerCertSANs:- etcd-host1- etcd-host2- etcd-host3- 10.129.6.211- 10.129.6.212- 10.129.6.213- 10.129.6.220featureGates: CoreDNS: trueimageRepository: &quot;devhub.beisencorp.com/google_containers&quot;EOF 初始化集群 1kubeadm init --config config.yaml 结果 12345678To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configas root: kubeadm join --token b99a00.a144ef80536d4344 10.129.6.220:6443 --discovery-token-ca-cert-hash sha256:ebc2f64e9bcb14639f26db90288b988c90efc43828829c557b6b66bbe6d68dfa 查看node 12345678910[root@etcd-host1 k8s]# kubectl get nodeNAME STATUS ROLES AGE VERSIONetcd-host1 noReady master 5h v1.9.0[root@etcd-host1 k8s]# kubectl get csNAME STATUS MESSAGE ERRORscheduler Healthy ok controller-manager Healthy ok etcd-1 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125; etcd-2 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125; etcd-0 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125; 问题记录 1234567891011如果使用kubeadm初始化集群，启动过程可能会卡在以下位置，那么可能是因为cgroup-driver参数与docker的不一致引起[apiclient] Created API client, waiting for the control plane to become readyjournalctl -t kubelet -S &apos;2017-06-08&apos;查看日志，发现如下错误error: failed to run Kubelet: failed to create kubelet: misconfiguration: kubelet cgroup driver: &quot;systemd&quot;需要修改KUBELET_CGROUP_ARGS=--cgroup-driver=systemd为KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfsvi /etc/systemd/system/kubelet.service.d/10-kubeadm.conf#Environment=&quot;KUBELET_CGROUP_ARGS=--cgroup-driver=systemd&quot;Environment=&quot;KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs&quot;systemctl daemon-reload &amp;&amp; systemctl restart kubelet 安装网络组件 podnetwork 我们选用kube-router 123wget https://github.com/cloudnativelabs/kube-router/blob/master/daemonset/kubeadm-kuberouter.yamlkubectl apply -f kubeadm-kuberouter.yaml 结果 123456789[root@etcd-host1 k8s]# kubectl get po --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system coredns-546545bc84-zc5dx 1/1 Running 0 6hkube-system kube-apiserver-etcd-host1 1/1 Running 0 6hkube-system kube-controller-manager-etcd-host1 1/1 Running 0 6hkube-system kube-proxy-pfj7x 1/1 Running 0 6hkube-system kube-router-858b7 1/1 Running 0 37mkube-system kube-scheduler-etcd-host1 1/1 Running 0 6h[root@etcd-host1 k8s]# 部署其他Master 节点 拷贝master01 配置 master02 master03 123456#拷贝pki 证书mkdir -p /etc/kubernetes/pkiscp -r root@10.129.6.211:/etc/kubernetes/pki /etc/kubernetes#拷贝初始化配置scp -r root@10.129.6.211://root/k8s/config.yaml /etc/kubernetes/config.yaml 初始化 master02 master03 12#初始化kubeadm init --config /etc/kubernetes/config.yaml -结果 1234567891011121314151617181920212223[root@etcd-host2 k8s]# kubectl get nodeNAME STATUS ROLES AGE VERSIONetcd-host1 Ready master 6h v1.9.0etcd-host2 Ready master 5m v1.9.0etcd-host3 Ready master 49s v1.9.0[root@etcd-host2 k8s]# kubectl get po --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system coredns-546545bc84-zc5dx 1/1 Running 0 6hkube-system kube-apiserver-etcd-host1 1/1 Running 0 6hkube-system kube-apiserver-etcd-host2 1/1 Running 0 5mkube-system kube-apiserver-etcd-host3 1/1 Running 0 46skube-system kube-controller-manager-etcd-host1 1/1 Running 0 6hkube-system kube-controller-manager-etcd-host2 1/1 Running 0 5mkube-system kube-controller-manager-etcd-host3 1/1 Running 0 46skube-system kube-proxy-gk95d 1/1 Running 0 51skube-system kube-proxy-mrzbq 1/1 Running 0 5mkube-system kube-proxy-pfj7x 1/1 Running 0 6hkube-system kube-router-7xsbn 1/1 Running 0 51skube-system kube-router-858b7 1/1 Running 0 47mkube-system kube-router-8hdh6 1/1 Running 0 5mkube-system kube-scheduler-etcd-host1 1/1 Running 0 6hkube-system kube-scheduler-etcd-host2 1/1 Running 0 5mkube-system kube-sche 参观文档12345678#k8s 官方文档https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/# kubeadm ha 项目文档https://github.com/indiketa/kubeadm-hahttps://github.com/cookeem/kubeadm-ha/blob/master/README_CN.mdhttps://medium.com/@bambash/ha-kubernetes-cluster-via-kubeadm-b2133360b198# kubespray 之前的kargo ansible项目https://github.com/kubernetes-incubator/kubespray/blob/master/docs/ha-mode.md","path":"2018/01/19/K8s-1-9实践-Kubeadm-HA-1-9-高可用-集群-本地离线部署/"},{"title":"[Kube-router 实践]K8s 1.9 Kube-router BGP外部对接发布内部路由 和 高可用","text":"实验背景新版本特性： k8s 1.9 做了很大组件性能改进 ， 本版本用kube-router组件取代kube-proxy,用lvs做svc负载均衡，更快稳定。 用coredns取代kube-dns，DNS更稳定。 经过测试1.9版，消除了以往的kubelet docker狂报错误日志的错误 ，更完美 支持 add动态插件 功能需求： 发布内部k8s网络，到机房全网 cluster-ip，external-ip 全网路由 解决iptables 性能和负载聚合问题 还有iptables 负载NAT 丢失源ip问题 1234567891011121314151617k8s测试版本kubeadm version: &amp;version. GitVersion:&quot;v1.9.0&quot;， BuildDate:&quot;2017-12-15T20:55:30Z&quot;网络设备Cisco 7200 R1 10.129.6.91R2 10.129.6.92Vrrp 10.129.6.8K8s Node网络node01 10.129.6.211node03 10.129.6.213K8s 网络10.244.0.0/16SVC 网络NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdefault kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 6dnginx01 ClusterIP 10.110.133.162 10.33.1.11 80/TCP 6d 网络拓扑如图： 网络高可用，目前我的方案是 双核心交换机跑VRRP 模拟器，模拟网络设备拓扑 R1 R2 主备路由器 1234567891011121314151617181920212223242526272829303132333435#R1 Cisco Configinterface FastEthernet0/0 ip address 10.129.6.91 255.255.255.0 vrrp 1 ip 10.129.6.8 vrrp 1 priority 150router bgp 64513 no synchronization bgp log-neighbor-changes neighbor 10.129.6.92 remote-as 64513 neighbor 10.129.6.211 remote-as 64512 neighbor 10.129.6.213 remote-as 64512 maximum-paths 2 no auto-summaryip route 0.0.0.0 0.0.0.0 10.129.6.1################################R2 Cisco Configinterface FastEthernet0/0 ip address 10.129.6.92 255.255.255.0 vrrp 1 ip 10.129.6.8 vrrp 1 priority 110 router bgp 64513 no synchronization bgp log-neighbor-changes neighbor 10.129.6.92 remote-as 64513 neighbor 10.129.6.211 remote-as 64512 neighbor 10.129.6.213 remote-as 64512 maximum-paths 2 no auto-summaryip route 0.0.0.0 0.0.0.0 10.129.6.1 部署k8s 1.9此处省略，测试使用所以用kubeadm 部署简单快速具体可以参看官方文档1https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/ 部署 kube-router体系结构Kube路由器是围绕观察者和控制器的概念而建立的。 观察者使用Kubernetes监视API来获取与创建，更新和删除Kubernetes对象有关的事件的通知。 每个观察者获取与特定API对象相关的通知。 在从API服务器接收事件时，观察者广播事件。 控制器注册以获取观察者的事件更新，并处理事件。Kube-router由3个核心控制器和多个观察者组成，如下图所示。主要改进 使用ipvs 替代 Iptables 方案 -性能更好 使用BGP 组网 更易发布和扩展对接 之前方案要使用caclio 其他方案 集成police 网络策略 之前网络层控制很弱 负载均衡 更多可选功能 如rr sip hash ip回话保持更多信息关注https://cloudnativelabs.github.io deployment部署Yaml文件在Kubernetes上部署Kube-router的最好的入门方法是使用集群安装程序. kubeadm 部署Kube-router，比较简单就是导入yaml文件即可我们使用的是如下，为了对接Cisco 网络设备发布路由 123wget https://github.com/cloudnativelabs/kube-router/blob/master/daemonset/kube-router-all-service-daemonset-advertise-routes.yamlkubectl apply -f kube-router-all-service-daemonset-advertise-routes.yaml 12345678910111213141516171819#我们根据实际环境修改下containers: - args: - --run-router=true #启用Pod网络 - 通过iBGP发布并学习到Pod的路由。 （默认为true） - --run-firewall=true #启用网络策略 - 设置iptables为pod提供入口防火墙。 （默认为true） - --run-service-proxy=true #启用服务代理 - 为Kubernetes服务设置IPVS。 （默认为true） - --advertise-cluster-ip=true #将该服务的集群IP添加到RIB，以便通告给BGP peers. - --advertise-external-ip=true #将服务的外部IP添加到RIB，以便将其通告给BGP peers. - --cluster-asn=64512 #集群自身节点运行iBGP的ASN编号. - --peer-router-ips=10.129.6.8 #所有节点将对等的外部路由器的IP地址，并通告集群ip和pod cidr。 （默认[]） - --peer-router-asns=64513 #集群节点将向其通告集群ip和节点的pid cidr的BGP peers的ASN编号。 （默认[]） 更多部署请自行查询 1https://github.com/cloudnativelabs/kube-router/tree/master/daemonset 部署的nginx 测试服务12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#apiVersion: v1kind: Servicemetadata: annotations: kube-router.io/service.scheduler: sh labels: app: nginx01 group: default role: master tier: backend name: nginx01 namespace: defaultspec: clusterIP: 10.110.133.162 externalIPs: - 10.33.1.11 ports: - port: 80 protocol: TCP targetPort: 80 selector: app: nginx01 group: default role: master tier: backend sessionAffinity: None type: ClusterIPstatus: loadBalancer: &#123;&#125;---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx01 # these labels can be applied automatically # from the labels in the pod template if not set # labels: # app: redis # role: master # tier: backend namespace: defaultspec: # this replicas value is default # modify it according to your case replicas: 1 # selector can be applied automatically # from the labels in the pod template if not set # selector: # matchLabels: # app: guestbook # role: master # tier: backend template: metadata: labels: app: nginx01 role: master tier: backend group: default spec: containers: - name: nginx01 image: devhub.beisencorp.com/test/nginx:v3 # or just image: redis resources: requests: cpu: 100m memory: 80Mi limits: cpu: 500m memory: 200M ports: - containerPort: 80 测试路由123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384[root@node01 k8s]#kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 6dnginx01 ClusterIP 10.110.133.162 10.33.1.11 80/TCP 6d[root@node01 k8s]# curl 10.33.1.11&lt; !DOCTYPE html&gt;&lt; html&gt;&lt; head&gt;&lt; meta charset=&quot;UTF-8&quot;&gt;&lt; title&gt; 主标题 | 副标题&lt; /title&gt;&lt; /head&gt;&lt; body&gt;&lt;p&gt;Hello, world! 我是版本V1My V2 e WorldZ&lt;/p&gt;&lt;p&gt;Hello, world! ^ ^ ^ ^ ^ ^ V1My V2 Bye WorldZ&lt;/p&gt;&lt;p&gt;Hello, world! ^ ^ ^ ^ ^ ^ V1My V2 Bye WorldZ&lt;/p&gt;&lt;p&gt;Hello, world! ^ ^ ^ ^ ^ ^ V1My V2 Bye WorldZ&lt;/p&gt;ZX&lt; /body&gt;&lt; /html&gt;## 我们查看下 网络设备路由器信息是否学习过来#BGP邻居已建立R1#sh bgp sum BGP router identifier 10.129.6.91, local AS number 64513Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd10.129.6.92 4 64513 147 146 25 0 0 02:16:05 610.129.6.211 4 64512 306 269 0 0 0 01:28:40 Active10.129.6.213 4 64512 314 270 0 0 0 01:28:40 ActiveR1#sh ip route 10.0.0.0/8 is variably subnetted, 7 subnets, 2 masksB 10.33.1.11/32 [200/0] via 10.129.6.92, 01:28:33B 10.110.133.162/32 [200/0] via 10.129.6.92, 01:28:33B 10.96.0.10/32 [200/0] via 10.129.6.92, 01:28:33B 10.96.0.1/32 [200/0] via 10.129.6.92, 01:28:33C 10.129.6.0/24 is directly connected, FastEthernet0/0B 10.244.0.0/24 [200/0] via 10.129.6.211, 01:28:33B 10.244.1.0/24 [200/0] via 10.129.6.213, 01:28:33S* 0.0.0.0/0 [1/0] via 10.129.6.1#R2 同上类同 R2备节点从R1主节点 学习全网路由### 我们在看下 kube-router邻居和路由是否学习过来Here&apos;s a quick look at what&apos;s happening on this Node--- BGP Server Configuration ---AS: ======64512======Router-ID: 10.129.6.211Listening Port: 179, Addresses: 0.0.0.0, ::--- BGP Neighbors ---Peer AS Up/Down State |#Received Accepted10.129.6.8 64513 00:00:15 Establ | 6 010.129.6.213 64512 03:40:40 Establ | 1 1--- BGP Route Info --- Network Next Hop AS_PATH Age Attrs*&gt; 10.33.1.11/32 10.129.6.211 00:00:58 [&#123;Origin: i&#125;]*&gt; 10.96.0.1/32 10.129.6.211 00:00:58 [&#123;Origin: i&#125;]*&gt; 10.96.0.10/32 10.129.6.211 00:00:58 [&#123;Origin: i&#125;]*&gt; 10.110.133.162/32 10.129.6.211 00:00:58 [&#123;Origin: i&#125;]*&gt; 10.244.0.0/24 10.129.6.211 00:00:58 [&#123;Origin: i&#125;]*&gt; 10.244.1.0/24 10.129.6.213 03:40:40 [&#123;Origin: i&#125; &#123;LocalPref: 100&#125;]--- IPVS Services ---IP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 10.33.1.11:80 sh -&gt; 10.244.1.16:80 Masq 1 0 0 TCP 10.96.0.1:443 rr persistent 10800 -&gt; 10.129.6.211:6443 Masq 1 0 0 TCP 10.96.0.10:53 rr -&gt; 10.244.0.13:53 Masq 1 0 0 TCP 10.110.133.162:80 sh -&gt; 10.244.1.16:80 Masq 1 0 0 UDP 10.96.0.10:53 rr -&gt; 10.244.0.13:53 Masq 1 0 0 NAME 我们找台机器加下静态路由 指向网络设备测试下 访问12345678# 获取 SVC 和pod ip[root@node01 k8s]# kubectl get svc -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 6d &lt;none&gt;nginx01 ClusterIP 10.110.133.162 10.33.1.11 80/TCP 6d app=nginx01,group=default,role=master,tier=backend[root@node01 k8s]# kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODEnginx01-d87b4fd74-7tw2x 1/1 Running 0 6d 10.244.1.16 node03 ClusterIP 路由1234567#ClusterIP 路由route add -net 10.96.0.0 netmask 255.255.0.0 gw 10.129.6.8ping 10.96.0.164 bytes from 10.96.0.1: icmp_seq=2 ttl=64 time=38.2 ms64 bytes from 10.96.0.1: icmp_seq=3 ttl=64 time=0.258 ms64 bytes from 10.96.0.1: icmp_seq=4 ttl=64 time=0.374 ms external-ip 路由1234567891011121314151617181920212223242526272829303132#external-ip 路由route add -net 10.33.1.11 netmask 255.255.255.255 gw 10.129.6.8[root@haproxy02 zeming]# ping 10.33.1.11PING 10.33.1.11 (10.33.1.11) 56(84) bytes of data.From 10.129.6.8: icmp_seq=1 Redirect Host(New nexthop: 10.129.6.211)64 bytes from 10.33.1.11: icmp_seq=1 ttl=64 time=41.4 ms^C--- 10.33.1.11 ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 755msrtt min/avg/max/mdev = 41.459/41.459/41.459/0.000 # 获取页面[root@haproxy02 zeming]# curl 10.33.1.11&lt; !DOCTYPE html&gt;&lt; html&gt;&lt; head&gt;&lt; meta charset=&quot;UTF-8&quot;&gt;&lt; title&gt; 主标题 | 副标题&lt; /title&gt;&lt; /head&gt;&lt; body&gt;&lt;p&gt;Hello, world! 我是版本V1My V2 e WorldZ&lt;/p&gt;&lt;p&gt;Hello, world! ^ ^ ^ ^ ^ ^ V1My V2 Bye WorldZ&lt;/p&gt;&lt;p&gt;Hello, world! ^ ^ ^ ^ ^ ^ V1My V2 Bye WorldZ&lt;/p&gt;&lt;p&gt;Hello, world! ^ ^ ^ ^ ^ ^ V1My V2 Bye WorldZ&lt;/p&gt;ZX&lt; /body&gt;&lt; /html&gt; pod 路由1234567#pod 路由route add -net 10.244.0.0 netmask 255.255.0.0 gw 10.129.6.8 [root@haproxy02 xuzeming]# ping 10.244.0.13PING 10.244.0.13 (10.244.0.13) 56(84) bytes of data.64 bytes from 10.244.0.13: icmp_seq=1 ttl=63 time=41.8 ms64 bytes from 10.244.0.13: icmp_seq=2 ttl=63 time=1.15 ms 主备网络设备BGP 切换测试 测试方式 关闭R1主节点网络设备 互联网口 制造 R1离线 观察 Vrrp 状态转移到备机 观察 BGP 备机 建立所有mesh 邻居状态 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051R2#*Mar 1 02:58:09.451: %VRRP-6-STATECHANGE: Fa0/0 Grp 1 state Backup -&gt; MasterR10#*Mar 1 02:58:46.415: %BGP-5-ADJCHANGE: neighbor 10.129.6.211 UpR10#*Mar 1 02:58:53.007: %BGP-5-ADJCHANGE: neighbor 10.129.6.213 Up#R10#sh ip rouCodes: C - connected, S - static, R - RIP, M - mobile, B - BGP D - EIGRP, EX - EIGRP external, O - OSPF, IA - OSPF inter area N1 - OSPF NSSA external type 1, N2 - OSPF NSSA external type 2 E1 - OSPF external type 1, E2 - OSPF external type 2 i - IS-IS, su - IS-IS summary, L1 - IS-IS level-1, L2 - IS-IS level-2 ia - IS-IS inter area, * - candidate default, U - per-user static route o - ODR, P - periodic downloaded static routeGateway of last resort is 10.129.6.1 to network 0.0.0.0 10.0.0.0/8 is variably subnetted, 7 subnets, 2 masksB 10.33.1.11/32 [20/0] via 10.129.6.213, 00:00:19 [20/0] via 10.129.6.211, 00:00:25B 10.110.133.162/32 [20/0] via 10.129.6.213, 00:00:19 [20/0] via 10.129.6.211, 00:00:25B 10.96.0.10/32 [20/0] via 10.129.6.213, 00:00:19 [20/0] via 10.129.6.211, 00:00:25B 10.96.0.1/32 [20/0] via 10.129.6.213, 00:00:19 [20/0] via 10.129.6.211, 00:00:27C 10.129.6.0/24 is directly connected, FastEthernet0/0B 10.244.0.0/24 [20/0] via 10.129.6.211, 00:00:27B 10.244.1.0/24 [20/0] via 10.129.6.213, 00:00:21S* 0.0.0.0/0 [1/0] via 10.129.6.1R10#sh bgp sumBGP router identifier 10.129.6.92, local AS number 64513BGP table version is 69, main routing table version 696 network entries using 702 bytes of memory10 path entries using 520 bytes of memory4 multipath network entries and 8 multipath paths3/1 BGP path/bestpath attribute entries using 372 bytes of memory1 BGP AS-PATH entries using 24 bytes of memory0 BGP route-map cache entries using 0 bytes of memory0 BGP filter-list cache entries using 0 bytes of memoryBGP using 1618 total bytes of memoryBGP activity 6/0 prefixes, 64/54 paths, scan interval 60 secsNeighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd10.129.6.91 4 64513 187 194 0 0 0 00:00:33 Active10.129.6.211 4 64512 683 364 69 0 0 00:02:38 510.129.6.213 4 64512 688 362 69 0 0 00:02:32 5 补充拓展kube-router 参数一栏1234567891011121314151617181920212223242526Usage of ./kube-router: --advertise-cluster-ip 将该服务的集群IP添加到RIB，以便通告给BGP peers. --advertise-external-ip 将服务的外部IP添加到RIB，以便将其通告给BGP peers. --cleanup-config 清理iptables规则，ipvs，ipset配置并退出. --cluster-asn uint 集群节点运行iBGP的ASN编号. --cluster-cidr string 群集中的CIDR范围。它被用来识别pods的范围. --config-sync-period duration apiserver配置同步之间的延迟（例如“5s”，“1m”）。必须大于0.（默认1m0s） --enable-overlay 当enable-overlay设置为true时，IP-in-IP隧道将用于跨不同子网中节点的pod-pod联网。如果设置为false，则不使用隧道，并且路由基础架构预计为不同子网中的节点之间的pod-pod联网路由流量（默认值为true） --enable-pod-egress 从Pod到群集外的SNAT流量。 （默认为true） --hairpin-mode 为每个服务端点添加iptable规则以支持流量管控. -h, --help 打印使用信息. --hostname-override string 覆盖节点的NodeName。如果kube-router无法自动确定您的NodeName，请设置此项. --iptables-sync-period duration iptables规则同步之间的延迟（例如&apos;5s&apos;，&apos;1m&apos;）。必须大于0.（默认1m0s） --ipvs-sync-period duration ipvs config同步之间的延迟（例如&apos;5s&apos;，&apos;1m&apos;，&apos;2h22m&apos;）。必须大于0.（默认1m0s） --kubeconfig string 具有授权信息的kubeconfig文件的路径（主位置由主标志设置）。 --masquerade-all SNAT所有流量到群集IP /节点端口。 --master string Kubernetes API服务器的地址（覆盖kubeconfig中的任何值）。 --nodeport-bindon-all-ip 对于NodePort类型的服务，创建监听节点的所有IP的IPVS服务. --nodes-full-mesh 集群中的每个节点都将建立与其他节点的BGP对等关系。 （默认为true） --peer-router-asns uintSlice 集群节点将向其通告集群ip和节点的pid cidr的BGP peers的ASN编号。 （默认[]） --peer-router-ips ipSlice 所有节点将对等的外部路由器的IP地址，并通告集群ip和pod cidr。 （默认[]） --peer-router-passwords stringSlice 用“--peer-router-ips”定义的BGP peers进行认证的密码。 --routes-sync-period duration 路线更新与广播之间的延迟（例如“5s”，“1m”，“2h22m”）。必须大于0.（默认1m0s） --run-firewall 启用网络策略 - 设置iptables为pod提供入口防火墙。 （默认为true） --run-router 启用Pod网络 - 通过iBGP发布并学习到Pod的路由。 （默认为true） --run-service-proxy 启用服务代理 - 为Kubernetes服务设置IPVS。 （默认为true） 123456789101112131415161718### DSR模式请阅读以下博客，了解如何结合使用DSR和“–advertise-external-ip”构建高度可扩展和可用的入口。 https://cloudnativelabs.github.io/post/2017-11-01-kube-high-available-ingress/您可以为每个服务启用DSR（直接服务器返回）功能。当启用的服务端点将直接响应客户端通过签署服务代理。启用DSR时，Kube-router将使用LVS的隧道模式来实现此功能。要启用DSR，您需要使用kube-router.io/service.dsr = tunnel注释来注释服务。例如，kubectl annotate service my-service &quot;kube-router.io/service.dsr=tunnel&quot;在当前的实现中，当在服务上应用注释时，DSR将仅适用于外部IP。此外，当使用DSR时，当前的实现不支持端口重新映射。所以你需要使用相同的端口和目标端口的服务你需要在kube-router守护进程清单中启用hostIPC：true和hostPID：true。并且必须将主路径/var/run/docker.sock设置为kube-router的一个volumemount。上述更改需要kube-router输入pod namespace，并在pod中创建ipip隧道，并将外部IP分配给VIP。对于示例清单，请查看启用DSR要求的[manifest]（../ daemonset / kubeadm-kuberouter-all-features-dsr.yaml）.### 负载均衡调度算法kube-router使用LVS作为服务代理。 LVS支持丰富的调度算法。您可以为该服务添加注释以选择一个调度算法。当一个服务没有注释时，默认情况下选择“轮询”调度策略 For least connection scheduling use:kubectl annotate service my-service “kube-router.io/service.scheduler=lc”For round-robin scheduling use:kubectl annotate service my-service “kube-router.io/service.scheduler=rr”For source hashing scheduling use:kubectl annotate service my-service “kube-router.io/service.scheduler=sh”For destination hashing scheduling use:kubectl annotate service my-service “kube-router.io/service.scheduler=dh”```参考文献 参考: https://github.com/cloudnativelabs/kube-router/tree/master/Documentation 译文: https://rocdu.io/2017/12/%E8%AF%91kube-router-documentation/ 转载请注明出处谢谢:http://xuzeming.top/2018/01/09/Kube-router-%E5%AE%9E%E8%B7%B5-K8s-1-9-Kube-router/","path":"2018/01/09/Kube-router-实践-K8s-1-9-Kube-router/"},{"title":"Today ","text":"Today 今天是美好一天，愿我们好好的预见你 遇见爱","path":"2017/12/21/news/"},{"title":"Nice To Meet U","text":"How are you","path":"2017/12/21/Nice-To-Meet-U/"}]}